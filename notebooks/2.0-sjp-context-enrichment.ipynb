{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random \n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# langchain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "DEPLOYMENT_NAME = os.environ[\"DEPLOYMENT_NAME\"]\n",
    "MODEL_NAME = \"gpt-35-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Load the full Autocast dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "DATA_FILE_NAME = \"autocast_questions.json\"\n",
    "DATA_PATH = os.path.join(DATA_DIR, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataframe shape: (6532, 14)\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6532 entries, 0 to 6531\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   question          6532 non-null   object             \n",
      " 1   id                6532 non-null   object             \n",
      " 2   background        6532 non-null   object             \n",
      " 3   publish_time      6532 non-null   datetime64[ns, UTC]\n",
      " 4   close_time        6532 non-null   object             \n",
      " 5   tags              6532 non-null   object             \n",
      " 6   source_links      6532 non-null   object             \n",
      " 7   prediction_count  6532 non-null   float64            \n",
      " 8   forecaster_count  6532 non-null   int64              \n",
      " 9   answer            3748 non-null   object             \n",
      " 10  choices           5126 non-null   object             \n",
      " 11  status            6532 non-null   object             \n",
      " 12  qtype             6532 non-null   object             \n",
      " 13  crowd             6532 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(1), object(11)\n",
      "memory usage: 714.6+ KB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Main dataframe shape: {full_df.shape}\")\n",
    "print(\"\")\n",
    "\n",
    "full_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Data\n",
    "\n",
    "Filter data to include only desired question types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_file_name(df, description=None):\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    df_size = df.shape[0]\n",
    "    file_name = f\"test_df-{current_time}-size_{df_size}\"\n",
    "    \n",
    "    if description is not None:\n",
    "        file_name += f\"-{description}\"\n",
    "    \n",
    "    file_ext = \".json\"\n",
    "    file_name += file_ext\n",
    "    \n",
    "    return file_name\n",
    "\n",
    "def save_test_df(df, file_name=None, data_dir=DATA_DIR, description=None):\n",
    "    if file_name is None:\n",
    "        file_name = build_file_name(df, description)\n",
    "    \n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    \n",
    "    df.to_json(file_path, orient=\"records\", lines=True)\n",
    "    print(f\"Saved test dataframe to: {file_path}\")\n",
    "\n",
    "def build_sample_df(df, qtypes=[\"t/f\", \"mc\"], n=100):\n",
    "    \"\"\"\n",
    "    Build a sample dataframe with a fixed number of samples per question type.\n",
    "    \"\"\"\n",
    "    sample_df = pd.DataFrame()\n",
    "    for qtype in qtypes:\n",
    "        sample_df = pd.concat([sample_df, df[df[\"qtype\"] == qtype].sample(n=n, random_state=1)])\n",
    "    sample_df = sample_df.sample(frac=1, random_state=1)\n",
    "    return sample_df\n",
    "\n",
    "def filter_df_by_vals(df: pd.DataFrame, filter_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters a dataframe based on a filter dictionary where the keys are dataframe\n",
    "    columns and the values are used to filter each column.\n",
    "\n",
    "    Args:\n",
    "        filter_dict (dict): filter dictionary\n",
    "        df (pd.DataFrame): dataframe\n",
    "    \"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    for key, value in filter_dict.items():\n",
    "        df_filtered = df_filtered[df_filtered[key] == value]\n",
    "    return df_filtered\n",
    "\n",
    "def filter_df_by_tag(df: pd.DataFrame, tag_col: str, filter_tag: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters a dataframe based on whether filter_tag appears in the list of tags\n",
    "    located in tag_col.\n",
    "\n",
    "    Args:\n",
    "        tag_col (str): tag column\n",
    "        filter_tag (str): tag\n",
    "        df (pd.DataFrame): dataframe\n",
    "    \"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    df_filtered = df_filtered[df_filtered[tag_col].apply(lambda x: filter_tag in x)]\n",
    "    return df_filtered\n",
    "\n",
    "def describe_question_row_by_idx(df, idx):\n",
    "    \"\"\"\n",
    "    Prints the question and choices for a given row in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe of questions.\n",
    "        idx (int): The index of the row to describe.\n",
    "    \"\"\"\n",
    "    row = df.iloc[idx]\n",
    "    print(f\"Question:\\n{row['question']}\\n\")\n",
    "    \n",
    "    print(\"Choices:\")\n",
    "    for i, choice in enumerate(row[\"choices\"]):\n",
    "        print(f\"{chr(65 + i)} - {choice}\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Crowd:\")\n",
    "    print(f\"Majority: {row['majority_pred']}\")\n",
    "    \n",
    "    lower_bound = row['acceptable_pred_lower_boundary']\n",
    "    upper_bound = row['acceptable_pred_upper_boundary']\n",
    "    print(f\"Acceptable range: ({round(lower_bound, 3)}, {round(upper_bound, 3)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (4276, 14)\n"
     ]
    }
   ],
   "source": [
    "# filter to only include true/false and multiple choice questions\n",
    "test_df = full_df[full_df[\"qtype\"].isin([\"t/f\", \"mc\"])]\n",
    "test_df = test_df.drop_duplicates(subset=[\"id\", \"question\"])\n",
    "\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (639, 14)\n"
     ]
    }
   ],
   "source": [
    "# build dataframe based on ids from existing filter file\n",
    "FILTER_DF_FILE_PATH = f\"{DATA_DIR}/autocast_filtered.json\"\n",
    "\n",
    "filter_df = pd.read_json(FILTER_DF_FILE_PATH)\n",
    "\n",
    "# filter df so qtype column is only t/f or mc\n",
    "filter_df = filter_df[filter_df[\"qtype\"].isin([\"t/f\", \"mc\"])]\n",
    "filter_df = filter_df[filter_df[\"choices\"].notna()]\n",
    "\n",
    "# filter test_df to only include ids from filter_df\n",
    "test_df = test_df[test_df[\"id\"].isin(filter_df[\"id\"])]\n",
    "test_df = test_df.drop_duplicates(subset=[\"id\", \"question\"])\n",
    "\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (98, 14)\n"
     ]
    }
   ],
   "source": [
    "# filter for active questions only\n",
    "test_df = test_df[test_df[\"status\"] == \"Active\"]\n",
    "test_df = test_df[test_df[\"choices\"].notna()]\n",
    "\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- OPTIONAL ----------\n",
    "\n",
    "# build random sample test dataframe\n",
    "\n",
    "# NUM_SAMPLES_PER_QTYPE = 150\n",
    "\n",
    "# test_df = build_sample_df(test_df, qtypes=[\"t/f\", \"mc\"], n=NUM_SAMPLES_PER_QTYPE)\n",
    "# test_df = test_df.drop_duplicates(subset=[\"id\", \"question\"])\n",
    "\n",
    "# print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_description = \"filtered_test_data\"\n",
    "\n",
    "TEST_DATA_FILE_NAME = build_file_name(test_df, test_data_description)\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIR, TEST_DATA_FILE_NAME)\n",
    "\n",
    "TEST_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test dataframe to: ../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json\n"
     ]
    }
   ],
   "source": [
    "save_test_df(test_df, TEST_DATA_FILE_NAME, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowd Prediction Boundaries\n",
    "\n",
    "Identify average / majority crowd predictions and set range of acceptable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_integify(dic_item):\n",
    "    \"\"\"\n",
    "    in case of multiple predictions submitted at the same time stamp, get their average\n",
    "    input: timestamp-forecast pair as one dictionary item in the crowd predictions\n",
    "    output: transformed dic item\n",
    "    \"\"\"\n",
    "    if isinstance(dic_item[\"forecast\"], int) or isinstance(dic_item[\"forecast\"], float):\n",
    "        return dic_item\n",
    "    else:\n",
    "        dic_item[\"forecast\"] = sum(dic_item[\"forecast\"])/len(dic_item[\"forecast\"])\n",
    "        return dic_item\n",
    "    \n",
    "def cal_boundary(qtype, value, dic_threshold, boundary):\n",
    "    if qtype == \"mc\" or qtype == \"t/f\":\n",
    "        if boundary == \"up\":\n",
    "            return value + dic_threshold[qtype]\n",
    "        else:\n",
    "            return value - dic_threshold[qtype]\n",
    "    else: #qtype = num\n",
    "        if boundary == \"up\":\n",
    "            return value * (1 + dic_threshold[qtype])\n",
    "        else:\n",
    "            return value * (1 - dic_threshold[qtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy range thresholds for each question type\n",
    "THRESHOLD_TF = 0.2\n",
    "THRESHOLD_MCQ = 0.1\n",
    "THRESHOLD_NUM = 0.1\n",
    "\n",
    "THRESHOLDS = {\n",
    "    \"t/f\": THRESHOLD_TF,\n",
    "    \"mc\": THRESHOLD_MCQ,\n",
    "    \"num\": THRESHOLD_NUM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"crowd\"] = test_df[\"crowd\"].apply(lambda x: [helper_integify(k) for k in x])\n",
    "col_all_crowd_predictions = test_df[\"crowd\"].apply(lambda x: [k[\"forecast\"] for k in x])\n",
    "\n",
    "test_df[\"avg_pred\"] = col_all_crowd_predictions.apply(lambda x: sum(x)/len(x))\n",
    "test_df[\"median_pred\"] = col_all_crowd_predictions.apply(lambda x: np.median(x))\n",
    "test_df[\"majority_pred\"] = col_all_crowd_predictions.apply(lambda x: max(set(x), key = x.count))\n",
    "test_df[\"pred_taken\"] = test_df[\"majority_pred\"]\n",
    "\n",
    "test_df[\"acceptable_pred_lower_boundary\"] = test_df.apply(lambda row: cal_boundary(row[\"qtype\"], row[\"majority_pred\"], THRESHOLDS, \"lower\"),\n",
    "                                                                  axis = 1)\n",
    "test_df[\"acceptable_pred_upper_boundary\"] = test_df.apply(lambda row: cal_boundary(row[\"qtype\"], row[\"majority_pred\"], THRESHOLDS, \"up\"),\n",
    "                                                                  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataframe shape: (98, 20)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>background</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>close_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_links</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>forecaster_count</th>\n",
       "      <th>answer</th>\n",
       "      <th>choices</th>\n",
       "      <th>status</th>\n",
       "      <th>qtype</th>\n",
       "      <th>crowd</th>\n",
       "      <th>avg_pred</th>\n",
       "      <th>median_pred</th>\n",
       "      <th>majority_pred</th>\n",
       "      <th>pred_taken</th>\n",
       "      <th>acceptable_pred_lower_boundary</th>\n",
       "      <th>acceptable_pred_upper_boundary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>G2124</td>\n",
       "      <td>CCTAs are a major concern for law enforcement ...</td>\n",
       "      <td>2021-09-17 15:49:41.402000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Society, Security and Conflict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>340.0</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-17 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>G2125</td>\n",
       "      <td>Tensions between Egypt, Ethiopia, and Sudan co...</td>\n",
       "      <td>2021-09-17 15:49:44.237000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, Security and Conflict, Non-US...</td>\n",
       "      <td>[https://www.securitycouncilreport.org/whatsin...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>59</td>\n",
       "      <td>None</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-17 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.085060</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>G2141</td>\n",
       "      <td>Recent media reporting indicates discussions r...</td>\n",
       "      <td>2021-09-30 14:38:57.362000+00:00</td>\n",
       "      <td>2023-01-01 08:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, US Politics, Security and Con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>188.0</td>\n",
       "      <td>51</td>\n",
       "      <td>None</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-30 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.171789</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question     id  \\\n",
       "1735  Will there be a complex coordinated terrorist ...  G2124   \n",
       "1736  Before 1 September 2022, will Egypt, Ethiopia,...  G2125   \n",
       "1750  Before 1 January 2023, will the Taipei Economi...  G2141   \n",
       "\n",
       "                                             background  \\\n",
       "1735  CCTAs are a major concern for law enforcement ...   \n",
       "1736  Tensions between Egypt, Ethiopia, and Sudan co...   \n",
       "1750  Recent media reporting indicates discussions r...   \n",
       "\n",
       "                         publish_time                 close_time  \\\n",
       "1735 2021-09-17 15:49:41.402000+00:00  2022-09-01 07:01:00+00:00   \n",
       "1736 2021-09-17 15:49:44.237000+00:00  2022-09-01 07:01:00+00:00   \n",
       "1750 2021-09-30 14:38:57.362000+00:00  2023-01-01 08:01:00+00:00   \n",
       "\n",
       "                                                   tags  \\\n",
       "1735                   [Society, Security and Conflict]   \n",
       "1736  [Foreign Policy, Security and Conflict, Non-US...   \n",
       "1750  [Foreign Policy, US Politics, Security and Con...   \n",
       "\n",
       "                                           source_links  prediction_count  \\\n",
       "1735                                                 []             340.0   \n",
       "1736  [https://www.securitycouncilreport.org/whatsin...             166.0   \n",
       "1750                                                 []             188.0   \n",
       "\n",
       "      forecaster_count answer    choices  status qtype  \\\n",
       "1735               111   None  [yes, no]  Active   t/f   \n",
       "1736                59   None  [yes, no]  Active   t/f   \n",
       "1750                51   None  [yes, no]  Active   t/f   \n",
       "\n",
       "                                                  crowd  avg_pred  \\\n",
       "1735  [{'timestamp': '2021-09-17 17:00:00+00:00', 'f...  0.051228   \n",
       "1736  [{'timestamp': '2021-09-17 17:00:00+00:00', 'f...  0.085060   \n",
       "1750  [{'timestamp': '2021-09-30 17:00:00+00:00', 'f...  0.171789   \n",
       "\n",
       "      median_pred  majority_pred  pred_taken  acceptable_pred_lower_boundary  \\\n",
       "1735         0.05           0.05        0.05                           -0.15   \n",
       "1736         0.05           0.05        0.05                           -0.15   \n",
       "1750         0.15           0.16        0.16                           -0.04   \n",
       "\n",
       "      acceptable_pred_upper_boundary  \n",
       "1735                            0.25  \n",
       "1736                            0.25  \n",
       "1750                            0.36  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Main dataframe shape: {test_df.shape}\")\n",
    "print(\"\")\n",
    "\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test dataframe to: ../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json\n"
     ]
    }
   ],
   "source": [
    "save_test_df(test_df, TEST_DATA_FILE_NAME, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal Context\n",
    "\n",
    "Add internal context by asking an LLM to research the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_background_research(question: str, chat, prompt) -> str:\n",
    "    \"\"\"Returns background research for the question\"\"\"    \n",
    "    context = chat(\n",
    "        prompt.format_prompt(question=question).to_messages()\n",
    "    ).content\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables for use with OpenAI API\n",
    "REQUEST_TIMEOUT = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a helpful research assistant. I will give you a question and your job is to \\\n",
    "provide me with background research about the question. You should tell me about the \\\n",
    "special terms in the question like entities, dates, locations, and other interesting \\\n",
    "terms. For each term, tell me the definition, historical background, and background on \\\n",
    "any important economic or societal considerations. You should also tell me about the \\\n",
    "relationship between the terms, how they fit together in history, and why one term might \\\n",
    "impact another. For each term, provide a rank of how important it is as part of the \\\n",
    "question. Do not attempt to answer the question. Only give me background research.\n",
    "\"\"\"\n",
    "\n",
    "context_system_message_prompt = SystemMessagePromptTemplate.from_template(CONTEXT_SYSTEM_TEMPLATE)\n",
    "\n",
    "context_human_template = \"\"\"\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "context_human_message_prompt = HumanMessagePromptTemplate.from_template(context_human_template)\n",
    "\n",
    "context_chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [context_system_message_prompt, context_human_message_prompt]\n",
    ")\n",
    "\n",
    "context_top_p = 0.9\n",
    "context_max_size = 2048\n",
    "\n",
    "context_chat = AzureChatOpenAI(\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        \"top_p\": context_top_p,\n",
    "    },\n",
    "    max_tokens=context_max_size,\n",
    "    request_timeout=REQUEST_TIMEOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Between 4 February 2022 and 31 December 2022, will a country formally progress in the NATO accession process?\n",
      "\n",
      "Choices:\n",
      "A - yes\n",
      "B - no\n",
      "\n",
      "Crowd:\n",
      "Majority: 0.55\n",
      "Acceptable range: (0.35, 0.75)\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 15\n",
    "sample_question = test_df.iloc[sample_idx][\"question\"]\n",
    "sample_choices = test_df.iloc[sample_idx][\"choices\"]\n",
    "\n",
    "describe_question_row_by_idx(test_df, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities:\n",
      "- NATO: The North Atlantic Treaty Organization is an intergovernmental military alliance between 30 North American and European countries. Its primary purpose is to guarantee the freedom and security of its member countries through political and military means.\n",
      "- Candidate country: A country that has expressed its desire to join NATO and has been granted the status of a candidate country by the alliance.\n",
      "\n",
      "Dates:\n",
      "- 4 February 2022: The starting date for the period in which a country could formally progress in the NATO accession process.\n",
      "- 31 December 2022: The ending date for the period in which a country could formally progress in the NATO accession process.\n",
      "\n",
      "Ranking:\n",
      "The most important term in this question is \"NATO accession process\" as it is the central theme of the question. The dates are also important as they specify the time frame for the question. The term \"candidate country\" is relevant as it refers to the countries that are in the process of joining NATO.\n",
      "\n",
      "Background:\n",
      "The NATO accession process is a series of steps that a country must take to become a member of the alliance. The process includes political and military reforms, as well as negotiations with NATO member countries. The process is open to any European country that meets the alliance's criteria for membership.\n",
      "\n",
      "The criteria for NATO membership include a commitment to democracy, the rule of law, and the protection of human rights. Candidate countries must also have stable institutions, a functioning market economy, and the ability to contribute to the alliance's missions.\n",
      "\n",
      "Since its creation in 1949, NATO has expanded several times, with the most recent additions being Montenegro in 2017 and North Macedonia in 2020. Currently, there are four candidate countries: Bosnia and Herzegovina, Georgia, North Macedonia, and Ukraine.\n",
      "\n",
      "The decision to allow a country to progress in the NATO accession process is made by the alliance's member countries. Each member has a veto, which means that all members must agree before a new country can join.\n",
      "\n",
      "Economic and societal considerations:\n",
      "Joining NATO can have significant economic benefits for candidate countries. It can lead to increased foreign investment, improved access to international markets, and greater economic stability. However, the process of joining NATO can also be costly, as it often requires significant investments in defense infrastructure.\n",
      "\n",
      "From a societal perspective, joining NATO can provide a sense of security and stability for citizens of candidate countries. It can also improve relations with neighboring countries and help to foster democratic reforms.\n",
      "\n",
      "Relationship between terms:\n",
      "The question asks whether a country will progress in the NATO accession process within a specific time frame. This process involves a candidate country meeting the criteria for membership, negotiating with NATO member countries, and ultimately being approved for membership. The decision to allow a country to progress in the process is made by the member countries, who consider a variety of factors, including political, military, and economic considerations.\n",
      "\n",
      "Why one term might impact another:\n",
      "The criteria for NATO membership are closely tied to a country's political and economic stability. If a country is experiencing political turmoil or economic instability, it may struggle to meet the criteria for membership and therefore may not be able to progress in the accession process. Additionally, the decision to allow a country to progress in the process is made by the member countries, who may take into account a candidate country's relations with neighboring countries or its alignment with other international organizations.\n"
     ]
    }
   ],
   "source": [
    "context_result = get_background_research(\n",
    "    sample_question,\n",
    "    chat=context_chat,\n",
    "    prompt=context_chat_prompt,\n",
    ")\n",
    "\n",
    "print(context_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/98 [03:20<18:21, 13.27s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 17%|█▋        | 17/98 [04:33<31:20, 23.21s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 31%|███       | 30/98 [07:47<15:47, 13.93s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 35%|███▍      | 34/98 [08:46<14:01, 13.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Azure has not provided the response due to a content filter being triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 61/98 [14:08<07:24, 12.01s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 64%|██████▍   | 63/98 [15:17<12:23, 21.24s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 74%|███████▍  | 73/98 [17:34<05:01, 12.06s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 76%|███████▌  | 74/98 [18:07<07:17, 18.24s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 77%|███████▋  | 75/98 [18:36<08:15, 21.53s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "100%|██████████| 98/98 [23:41<00:00, 14.51s/it]\n"
     ]
    }
   ],
   "source": [
    "context_data = []\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    id = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    error_count = 0\n",
    "    try:\n",
    "        context_result = get_background_research(\n",
    "            question,\n",
    "            chat=context_chat,\n",
    "            prompt=context_chat_prompt,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"Error count: {error_count}\")\n",
    "        print(e)\n",
    "        context_result = \"\"\n",
    "    \n",
    "    context_data.append({\n",
    "        \"id\": id,\n",
    "        \"question\": question,\n",
    "        \"context\": context_result\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_df = pd.DataFrame(context_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (98, 21)\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.merge(context_df, on=[\"id\", \"question\"])\n",
    "\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>background</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>close_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_links</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>forecaster_count</th>\n",
       "      <th>answer</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>qtype</th>\n",
       "      <th>crowd</th>\n",
       "      <th>avg_pred</th>\n",
       "      <th>median_pred</th>\n",
       "      <th>majority_pred</th>\n",
       "      <th>pred_taken</th>\n",
       "      <th>acceptable_pred_lower_boundary</th>\n",
       "      <th>acceptable_pred_upper_boundary</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>G2124</td>\n",
       "      <td>CCTAs are a major concern for law enforcement ...</td>\n",
       "      <td>2021-09-17 15:49:41.402000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Society, Security and Conflict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>340.0</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-17 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.051228</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Complex coordinated terrorist att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>G2125</td>\n",
       "      <td>Tensions between Egypt, Ethiopia, and Sudan co...</td>\n",
       "      <td>2021-09-17 15:49:44.237000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, Security and Conflict, Non-US...</td>\n",
       "      <td>[https://www.securitycouncilreport.org/whatsin...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>59</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-17 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.085060</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Egypt: a country in North Africa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>G2141</td>\n",
       "      <td>Recent media reporting indicates discussions r...</td>\n",
       "      <td>2021-09-30 14:38:57.362000+00:00</td>\n",
       "      <td>2023-01-01 08:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, US Politics, Security and Con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>188.0</td>\n",
       "      <td>51</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Active</td>\n",
       "      <td>t/f</td>\n",
       "      <td>[{'timestamp': '2021-09-30 17:00:00+00:00', 'f...</td>\n",
       "      <td>0.171789</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.36</td>\n",
       "      <td>Entities:\\n- Taipei Economic and Cultural Repr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     id  \\\n",
       "0  Will there be a complex coordinated terrorist ...  G2124   \n",
       "1  Before 1 September 2022, will Egypt, Ethiopia,...  G2125   \n",
       "2  Before 1 January 2023, will the Taipei Economi...  G2141   \n",
       "\n",
       "                                          background  \\\n",
       "0  CCTAs are a major concern for law enforcement ...   \n",
       "1  Tensions between Egypt, Ethiopia, and Sudan co...   \n",
       "2  Recent media reporting indicates discussions r...   \n",
       "\n",
       "                      publish_time                 close_time  \\\n",
       "0 2021-09-17 15:49:41.402000+00:00  2022-09-01 07:01:00+00:00   \n",
       "1 2021-09-17 15:49:44.237000+00:00  2022-09-01 07:01:00+00:00   \n",
       "2 2021-09-30 14:38:57.362000+00:00  2023-01-01 08:01:00+00:00   \n",
       "\n",
       "                                                tags  \\\n",
       "0                   [Society, Security and Conflict]   \n",
       "1  [Foreign Policy, Security and Conflict, Non-US...   \n",
       "2  [Foreign Policy, US Politics, Security and Con...   \n",
       "\n",
       "                                        source_links  prediction_count  \\\n",
       "0                                                 []             340.0   \n",
       "1  [https://www.securitycouncilreport.org/whatsin...             166.0   \n",
       "2                                                 []             188.0   \n",
       "\n",
       "   forecaster_count answer  ...  status qtype  \\\n",
       "0               111   None  ...  Active   t/f   \n",
       "1                59   None  ...  Active   t/f   \n",
       "2                51   None  ...  Active   t/f   \n",
       "\n",
       "                                               crowd  avg_pred  median_pred  \\\n",
       "0  [{'timestamp': '2021-09-17 17:00:00+00:00', 'f...  0.051228         0.05   \n",
       "1  [{'timestamp': '2021-09-17 17:00:00+00:00', 'f...  0.085060         0.05   \n",
       "2  [{'timestamp': '2021-09-30 17:00:00+00:00', 'f...  0.171789         0.15   \n",
       "\n",
       "   majority_pred  pred_taken  acceptable_pred_lower_boundary  \\\n",
       "0           0.05        0.05                           -0.15   \n",
       "1           0.05        0.05                           -0.15   \n",
       "2           0.16        0.16                           -0.04   \n",
       "\n",
       "   acceptable_pred_upper_boundary  \\\n",
       "0                            0.25   \n",
       "1                            0.25   \n",
       "2                            0.36   \n",
       "\n",
       "                                             context  \n",
       "0  Entities:\\n- Complex coordinated terrorist att...  \n",
       "1  Entities:\\n- Egypt: a country in North Africa,...  \n",
       "2  Entities:\\n- Taipei Economic and Cultural Repr...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test dataframe to: ../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json\n"
     ]
    }
   ],
   "source": [
    "save_test_df(test_df, TEST_DATA_FILE_NAME, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Prompt Examples\n",
    "\n",
    "Use active prompt to select the best examples for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forecaster():\n",
    "    def __init__(self, chat, prompt: str):\n",
    "        \"\"\"\n",
    "        A class for forecasting answers to questions using a language model.\n",
    "\n",
    "        Args:\n",
    "            chat: A chat language model.\n",
    "            prompt (str): A prompt for the language model.\n",
    "        \"\"\"\n",
    "        self.chat = chat\n",
    "        self.prompt = prompt\n",
    "    \n",
    "    def format_choices(self, choices: list[str]):\n",
    "        \"\"\"\n",
    "        Accepts a list of choices and formats them for the language model\n",
    "        with a letter for each choice and one choice per line.\n",
    "\n",
    "        Args:\n",
    "            choices (list): A list of choices.\n",
    "        \"\"\"\n",
    "        formatted_choices = \"\\n\".join(\n",
    "            [f\"{chr(65 + idx)} - {choice}\" for idx, choice in enumerate(choices)]\n",
    "        )\n",
    "        return formatted_choices\n",
    "    \n",
    "    def valid_answer(self, answer: str):\n",
    "        \"\"\"\n",
    "        Returns True if the answer is valid, False otherwise.\n",
    "\n",
    "        Args:\n",
    "            answer (str): An answer.\n",
    "        \"\"\"\n",
    "        # invalid if answer is not a string or is None\n",
    "        if not isinstance(answer, str) or answer is None:\n",
    "            return False\n",
    "\n",
    "        # invalid if first character is not a letter\n",
    "        if not re.match(r\"[A-Za-z]\", answer[0]):\n",
    "            return False\n",
    "        \n",
    "        # invalid if second character is a letter\n",
    "        if len(answer) > 1 and re.match(r\"[A-Za-z]\", answer[1]):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_answer(self, answer: str):\n",
    "        \"\"\"\n",
    "        If the answer is valid, returns just the first letter of the answer.\n",
    "        Converts a yes/no answer to a letter, where yes is A and no is B.\n",
    "        Returns None if the answer is not valid.\n",
    "\n",
    "        Args:\n",
    "            answer (str): An answer.\n",
    "        \"\"\"\n",
    "        if not self.valid_answer(answer):\n",
    "            return None\n",
    "        \n",
    "        answer = self.yes_no_to_letter(answer)\n",
    "\n",
    "        return answer[0]\n",
    "    \n",
    "    def idx_from_letter(self, letter: str):\n",
    "        \"\"\"\n",
    "        Converts a letter (A, B, C, etc.) to an index, where A is 0, \n",
    "        B is 1, and so on. Returns None if the letter is not valid.\n",
    "\n",
    "        Args:\n",
    "            letter (str): A letter.\n",
    "        \"\"\"\n",
    "        if letter is None:\n",
    "            return None\n",
    "        \n",
    "        letter = letter.upper()\n",
    "        letter = letter.strip()\n",
    "        letter = re.sub(r\"[^A-Z]\", \"\", letter)\n",
    "\n",
    "        # return None if letter is not valid\n",
    "        if len(letter) != 1 or ord(letter) < 65 or ord(letter) > 90:\n",
    "            return None\n",
    "\n",
    "        return int(ord(letter) - 65)\n",
    "    \n",
    "    def yes_no_to_letter(self, answer: str):\n",
    "        \"\"\"\n",
    "        Converts a yes/no answer to a letter, where yes is A and no is B.\n",
    "\n",
    "        Args:\n",
    "            answer (str): A yes/no answer.\n",
    "        \"\"\"\n",
    "        if answer is None:\n",
    "            return answer\n",
    "        elif answer.lower() == \"yes\" or answer.lower() == \"y\":\n",
    "            return \"A\"\n",
    "        elif answer.lower() == \"no\" or answer.lower() == \"n\":\n",
    "            return \"B\"\n",
    "        else:\n",
    "            return answer\n",
    "        \n",
    "    def get_llm_response(self, vars: dict):\n",
    "        \"\"\"\n",
    "        Returns a response from the language model in the form of a dictionary\n",
    "        with the original prediction and the cleaned predicted answer.\n",
    "\n",
    "        Args:\n",
    "            vars (dict): A dictionary with the question and choices.\n",
    "        \"\"\"\n",
    "        original_prediction = self.chat(\n",
    "            self.prompt.format_prompt(**vars).to_messages()\n",
    "        ).content\n",
    "\n",
    "        predicted_answer = self.clean_answer(original_prediction)\n",
    "        \n",
    "        return {\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"predicted_answer\": predicted_answer\n",
    "        }\n",
    "\n",
    "    def predict(self, question: str, choices: list[str]):\n",
    "        \"\"\"\n",
    "        Uses the language model to predict an answer to a question and returns\n",
    "        a dictionary with the question, choices, and predicted answer.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question.\n",
    "            choices (list): A list of choices.\n",
    "        \"\"\"\n",
    "        vars = {\n",
    "            \"question\": question,\n",
    "            \"choices\": self.format_choices(choices),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            results = self.get_llm_response(vars)\n",
    "            original_prediction = results[\"original_prediction\"]\n",
    "            predicted_answer = results[\"predicted_answer\"]\n",
    "            predicted_answer_idx = self.idx_from_letter(predicted_answer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            original_prediction = None\n",
    "            predicted_answer = None\n",
    "            predicted_answer_idx = None\n",
    "\n",
    "        if predicted_answer_idx is None or predicted_answer_idx >= len(choices):\n",
    "            predicted_answer = None\n",
    "            predicted_answer_text = None\n",
    "            predicted_answer_idx = None\n",
    "        else:\n",
    "            predicted_answer_text = choices[predicted_answer_idx]\n",
    "\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"predicted_answer_idx\": predicted_answer_idx,\n",
    "            \"predicted_answer_text\": predicted_answer_text\n",
    "        }\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def predict_df(self, df: pd.DataFrame, id_col: str, question_col: str, choices_col: str):\n",
    "        \"\"\"\n",
    "        Uses the language model to predict answers to a dataframe of questions.\n",
    "        Returns a dataframe with the question id, question, choices, and predicted answer.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): A dataframe of questions.\n",
    "            id_col (str): The name of the column containing the question IDs.\n",
    "            question_col (str): The name of the column containing the questions.\n",
    "            choices_col (str): The name of the column containing the choices.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            question = row[question_col]\n",
    "            choices = row[choices_col]\n",
    "            prediction = self.predict(question, choices)\n",
    "            prediction[id_col] = row[id_col]\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "        return predictions_df\n",
    "    \n",
    "class ForecasterTopK(Forecaster):\n",
    "    def __init__(self, chat, system_prompt: str, human_prompt: str, k: int = 10):\n",
    "        \"\"\"\n",
    "        A version of the Forecaster class that uses the top-k sampling method\n",
    "        for generating predictions.\n",
    "\n",
    "        Args:\n",
    "            chat: A chat language model.\n",
    "            system_prompt (str): A prompt for the language model.\n",
    "            human_prompt (str): A prompt for the language model.\n",
    "            k (int): The number of predictions to generate.\n",
    "        \"\"\"\n",
    "        self.chat = chat\n",
    "\n",
    "        self.system_prompt = system_prompt\n",
    "        self.human_prompt = human_prompt\n",
    "\n",
    "        self.k = k\n",
    "        self.chat.n = self.k\n",
    "\n",
    "    def get_llm_response(self, vars: dict):\n",
    "        \"\"\"\n",
    "        Returns a response from the language model in the form of a dictionary\n",
    "        with the original prediction and the cleaned predicted answer.\n",
    "\n",
    "        Args:\n",
    "            vars (dict): A dictionary with the question and choices.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_messages = [\n",
    "            [\n",
    "                self.system_prompt.format_messages()[0],\n",
    "                self.human_prompt.format_messages(**vars)[0],\n",
    "            ],\n",
    "        ]\n",
    "        result = self.chat.generate(batch_messages)\n",
    "\n",
    "        original_prediction = [generation.text for generation in result.generations[0]]\n",
    "\n",
    "        predicted_answers = [self.clean_answer(prediction) \n",
    "                             for prediction in original_prediction]\n",
    "        \n",
    "        return {\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"top_k_predicted_answers\": predicted_answers,\n",
    "            \"predicted_answer\": self.majority_choice(predicted_answers)\n",
    "        }\n",
    "    \n",
    "    def majority_choice(self, predicted_answers: list[str]):\n",
    "        \"\"\"\n",
    "        Returns the majority choice from a list of predicted answers.\n",
    "\n",
    "        Args:\n",
    "            predicted_answers (list): A list of predicted answers.\n",
    "        \"\"\"\n",
    "        counts = {answer: 0 for answer in predicted_answers}\n",
    "        for answer in predicted_answers:\n",
    "            counts[answer] += 1\n",
    "        majority_answer = max(counts, key=counts.get)\n",
    "        return majority_answer\n",
    "\n",
    "    def predict(self, question: str, choices: list[str]):\n",
    "        \"\"\"\n",
    "        Uses the language model to predict an answer to a question and returns\n",
    "        a dictionary with the question, choices, and predicted answer.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question.\n",
    "            choices (list): A list of choices.\n",
    "        \"\"\"\n",
    "        vars = {\n",
    "            \"question\": question,\n",
    "            \"choices\": self.format_choices(choices),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            results = self.get_llm_response(vars)\n",
    "            original_prediction = results[\"original_prediction\"]\n",
    "            predicted_answer = results[\"predicted_answer\"]\n",
    "            predicted_answer_idx = self.idx_from_letter(predicted_answer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            original_prediction = None\n",
    "            predicted_answer = None\n",
    "            predicted_answer_idx = None\n",
    "\n",
    "        if predicted_answer_idx is None or predicted_answer_idx >= len(choices):\n",
    "            predicted_answer = None\n",
    "            predicted_answer_text = None\n",
    "            predicted_answer_idx = None\n",
    "        else:\n",
    "            predicted_answer_text = choices[predicted_answer_idx]\n",
    "\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"predicted_answer_idx\": predicted_answer_idx,\n",
    "            \"predicted_answer_text\": predicted_answer_text,\n",
    "            \"top_k_predicted_answers\": results[\"top_k_predicted_answers\"]\n",
    "        }\n",
    "\n",
    "        return response\n",
    "\n",
    "class ForecasterTopKUncertainty(ForecasterTopK):\n",
    "    def __init__(self, chat, system_prompt: str, human_prompt: str, k: int = 10):\n",
    "        \"\"\"\n",
    "        A version of the Forecaster class that uses the top-k sampling method\n",
    "        for generating predictions.\n",
    "\n",
    "        Args:\n",
    "            chat: A chat language model.\n",
    "            system_prompt (str): A prompt for the language model.\n",
    "            human_prompt (str): A prompt for the language model.\n",
    "            example_prompt (str): A prompt for the language model.\n",
    "            k (int): The number of predictions to generate.\n",
    "        \"\"\"\n",
    "        self.chat = chat\n",
    "\n",
    "        self.system_prompt = system_prompt\n",
    "        self.human_prompt = human_prompt\n",
    "\n",
    "        self.k = k\n",
    "        self.chat.n = self.k\n",
    "\n",
    "    def generate_example(self, question: str, choices: list[str], answer: str):\n",
    "        \"\"\"\n",
    "        Generates an example for the language model.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question.\n",
    "            choices (list): A list of choices.\n",
    "            answer (str): An answer.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of messages, consisting of a human message and an AI message.\n",
    "        \"\"\"\n",
    "        vars = {\n",
    "            \"question\": question,\n",
    "            \"choices\": self.format_choices(choices),\n",
    "        }\n",
    "\n",
    "        human_message = self.human_prompt.format_messages(**vars)[0]\n",
    "        \n",
    "        formatted_answer = self.yes_no_to_letter(answer)\n",
    "        ai_message = AIMessage(content=formatted_answer)\n",
    "\n",
    "        return [human_message, ai_message]\n",
    "\n",
    "    def get_llm_response_with_example(self, vars: dict, examples: list):\n",
    "        \"\"\"\n",
    "        Returns a response from the language model in the form of a dictionary\n",
    "        with the original prediction and the cleaned predicted answer.\n",
    "\n",
    "        Args:\n",
    "            vars (dict): A dictionary with the question and choices.\n",
    "            examples (list): A list of examples, consisting of user messages\n",
    "                and assistant messages.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_messages = [\n",
    "            [self.system_prompt.format_messages()[0]] + \\\n",
    "            [AIMessage(content=\"Sure, I'd love to help!\")] + \\\n",
    "            examples + \\\n",
    "            [self.human_prompt.format_messages(**vars)[0]]\n",
    "        ]\n",
    "        result = self.chat.generate(batch_messages)\n",
    "\n",
    "        original_prediction = [generation.text for generation in result.generations[0]]\n",
    "\n",
    "        predicted_answers = [self.clean_answer(prediction) \n",
    "                             for prediction in original_prediction]\n",
    "        \n",
    "        return {\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"top_k_predicted_answers\": predicted_answers,\n",
    "            \"predicted_answer\": self.majority_choice(predicted_answers)\n",
    "        }\n",
    "    \n",
    "    def generate_n_examples_with_uncertainty(self, question: str, choices: list[str], qtype: str,\n",
    "                                             tags: list[str], example_df, n: int, random=True) -> dict:\n",
    "        \"\"\"\n",
    "        Generates n examples for use in predicting answers to the question along with\n",
    "        uncertainty ratings for each example. The examples are drawn from example_df,\n",
    "        which must have question, choices, and answer columns (meaning the questions must\n",
    "        be resolved and have a real answer). If random is True, the examples are randomly\n",
    "        sampled from example_df. Otherwise, the examples are the first n in the dataframe.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question.\n",
    "            choices (list): A list of choices.\n",
    "            qtype (str): The type of question.\n",
    "            tags (list): A list of tags.\n",
    "            example_df (pd.DataFrame): A dataframe of resolved questions with answers.\n",
    "            n (int): The number of examples to generate.\n",
    "            random (bool): Whether to randomly sample the examples from example_df.\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary with the examples, with the keys example_question, example_choices, \n",
    "            example_answer, majority_predicted_answer, and uncertainty.\n",
    "        \"\"\"\n",
    "        eligible_examples = example_df[example_df[\"qtype\"] == qtype]\n",
    "\n",
    "        tag_match = lambda x: len(set(x).intersection(tags)) > 0\n",
    "        eligible_examples = eligible_examples[eligible_examples[\"tags\"].apply(tag_match)]\n",
    "\n",
    "        if random:\n",
    "            eligible_examples = eligible_examples.sample(n=n)\n",
    "        else:\n",
    "            eligible_examples = eligible_examples.head(n)\n",
    "        \n",
    "        examples = []\n",
    "        for _, row in eligible_examples.iterrows():\n",
    "            example_question = row[\"question\"]\n",
    "            example_choices = row[\"choices\"]\n",
    "            example_answer = row[\"answer\"]\n",
    "            example = self.generate_example(example_question, example_choices, example_answer)\n",
    "\n",
    "            vars = {\n",
    "                \"question\": question,\n",
    "                \"choices\": self.format_choices(choices),\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                result = self.get_llm_response_with_example(vars, example)\n",
    "                uncertainty, majority_predicted_answer = self.calculate_uncertainty(\n",
    "                    result[\"top_k_predicted_answers\"]\n",
    "                )\n",
    "                full_example = {\n",
    "                    \"example_question\": example_question,\n",
    "                    \"example_choices\": example_choices,\n",
    "                    \"example_answer\": example_answer,\n",
    "                    \"majority_predicted_answer\": majority_predicted_answer,\n",
    "                    \"uncertainty\": uncertainty,\n",
    "                }\n",
    "                examples.append(full_example)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                full_example = {\n",
    "                    \"example_question\": example_question,\n",
    "                    \"example_choices\": example_choices,\n",
    "                    \"example_answer\": example_answer,\n",
    "                    \"majority_predicted_answer\": None,\n",
    "                    \"uncertainty\": None,\n",
    "                }\n",
    "                examples.append(full_example)\n",
    "\n",
    "        return examples\n",
    "    \n",
    "    def convert_example_results_to_df(self, example_uncertainty_results: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts the results of generate_n_examples_with_uncertainty to a dataframe.\n",
    "\n",
    "        Args:\n",
    "            example_uncertainty_results (dict): results of generate_n_examples_with_uncertainty\n",
    "        \"\"\"\n",
    "        example_uncertainty_results_df = pd.DataFrame(example_uncertainty_results)\n",
    "        desired_cols = [\"example_question\", \"example_choices\", \"example_answer\", \n",
    "                        \"uncertainty\", \"majority_predicted_answer\"]\n",
    "        example_uncertainty_results_df = example_uncertainty_results_df[desired_cols]\n",
    "        return example_uncertainty_results_df\n",
    "\n",
    "    def calculate_uncertainty(self, answers: list[str]) -> tuple:\n",
    "        \"\"\"\n",
    "        Calculates the uncertainty of a list of answers. Uncertainty is defined as the\n",
    "        rate of disagreements. To calculate, get the majority answer, and get the number\n",
    "        of answers that is different from majority answer, divided by total number of answers.\n",
    "\n",
    "        Args:\n",
    "            answers (list): list of answers\n",
    "\n",
    "        Returns:\n",
    "            tuple: uncertainty, most common answer\n",
    "        \"\"\"\n",
    "        value, count = Counter(answers).most_common()[0]\n",
    "        uncertainty = 1 - count / len(answers)\n",
    "        return (uncertainty, value)\n",
    "    \n",
    "    def predict(self, question: str, choices: list[str], example: dict):\n",
    "        \"\"\"\n",
    "        Uses the language model to predict an answer to a question and returns\n",
    "        a dictionary with the question, choices, context, and predicted answer.\n",
    "\n",
    "        Args:\n",
    "            question (str): A question.\n",
    "            choices (list): A list of choices.\n",
    "            example (dict): An example.\n",
    "        \"\"\"\n",
    "        vars = {\n",
    "            \"question\": question,\n",
    "            \"choices\": self.format_choices(choices)\n",
    "        }\n",
    "\n",
    "        example_exchange = self.generate_example(\n",
    "            question=example[\"example_question\"],\n",
    "            choices=example[\"example_choices\"],\n",
    "            answer=example[\"example_answer\"]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            results = self.get_llm_response_with_example(vars, example_exchange)\n",
    "            original_prediction = results[\"original_prediction\"]\n",
    "            predicted_answer = results[\"predicted_answer\"]\n",
    "            predicted_answer_idx = self.idx_from_letter(predicted_answer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            original_prediction = None\n",
    "            predicted_answer = None\n",
    "            predicted_answer_idx = None\n",
    "\n",
    "        if predicted_answer_idx is None or predicted_answer_idx >= len(choices):\n",
    "            predicted_answer = None\n",
    "            predicted_answer_text = None\n",
    "            predicted_answer_idx = None\n",
    "        else:\n",
    "            predicted_answer_text = choices[predicted_answer_idx]\n",
    "\n",
    "        response = {\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"original_prediction\": original_prediction,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"predicted_answer_idx\": predicted_answer_idx,\n",
    "            \"predicted_answer_text\": predicted_answer_text,\n",
    "            \"top_k_predicted_answers\": results[\"top_k_predicted_answers\"]\n",
    "        }\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def predict_df(self, df: pd.DataFrame, id_col: str, question_col: str, choices_col: str, example_col: str):\n",
    "        \"\"\"\n",
    "        Uses the language model to predict answers to a dataframe of questions.\n",
    "        Returns a dataframe with the question id, question, choices, and predicted answer.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): A dataframe of questions.\n",
    "            id_col (str): The name of the column containing the question IDs.\n",
    "            question_col (str): The name of the column containing the questions.\n",
    "            choices_col (str): The name of the column containing the choices.\n",
    "            example_col (str): The name of the column containing the examples.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            question = row[question_col]\n",
    "            choices = row[choices_col]\n",
    "            example = row[example_col]\n",
    "\n",
    "            error_count = 0\n",
    "            try:\n",
    "                prediction = self.predict(question, choices, example)\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"Error on question: {question}\")\n",
    "                print(f\"Error count: {error_count}\")\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            prediction[id_col] = row[id_col]\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "        return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tags: 41\n"
     ]
    }
   ],
   "source": [
    "# pull tags from test_df\n",
    "test_tags = test_df[\"tags\"].tolist()\n",
    "\n",
    "# flatten test_tags\n",
    "test_tags = [item for sublist in test_tags for item in sublist]\n",
    "\n",
    "# remove duplicates\n",
    "test_tags = sorted(list(set(test_tags)))\n",
    "\n",
    "print(f\"Number of unique tags: {len(test_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataframe shape: (2187, 14)\n"
     ]
    }
   ],
   "source": [
    "# build sample_df from resolved questions with matching qtype and tags as test_df\n",
    "sample_df = full_df[full_df[\"status\"] == \"Resolved\"]\n",
    "sample_df = sample_df[sample_df[\"qtype\"].isin([\"mc\", \"t/f\"])]\n",
    "sample_df = sample_df[sample_df[\"tags\"].apply(lambda x: len(set(x).intersection(test_tags)) > 0)]\n",
    "\n",
    "print(f\"Sample dataframe shape: {sample_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "META_PROMPT = \"\"\"\n",
    "I am working on a very important forecasting project and you are my helpful forecasting assistant. I will give you a multiple choice question about the future and you should return to me the letter that corresponds to the best choice. Some of the questions are very difficult, but it's still very important to choose the best possible answer from the provided choices. You must never refuse to provide an answer, even if the question is impossible to answer with certainty. If you are not sure about the answer, make your best guess based on available information.\n",
    "\n",
    "To identify the best possible choice, start by making sure you understand the question. If I have background research, I will provide it to you and you should use that information to better understand the question. If I do not provide background research, then you should think step by step about the relationship between different entities and terms in the question. \n",
    "\n",
    "In some cases, I will show you past examples of questions and the correct answers. If you have seen past examples with correct answers, then you can use those to practice and learn how to answer questions accurately. Start by pretending you need to answer those questions. Think step by step through the example question and example choices and try to understand everything about them. Choose what you think is the best answer to the example question and then compare it to the real best answer. If your answer is correct, remember the process you used and apply it to future questions. If your answer is wrong, then think about what you should do differently to get the correct answer. However, remember that every question is independent and the answers to example questions may not be relevant to new questions. It is only the process for finding the best answer that is the same.\n",
    "\n",
    "I will give you the questions in the below format, where the question, choices, and answer are in double brackets. If I have background research, I will include it in the optional section demarcated by +++.\n",
    "\n",
    "Question:\n",
    "{{the question to answer}}\n",
    "\n",
    "Choices:\n",
    "A - {{choice 1}}\n",
    "B - {{choice 2}}\n",
    "...\n",
    "n - {{choice n}}\n",
    "\n",
    "+++\n",
    "Background Research:\n",
    "{{useful information about the question}}\n",
    "+++\n",
    "\n",
    "{{letter that corresponds to best choice}}\n",
    "\n",
    "It is extremely important to only give me the letter of the best answer and nothing else. You must never refuse to provide an answer, no matter what. If the best answer is choice 1 then respond \"A\", if the best answer is choice 2 then respond \"B\", if the best answer is choice n then respond \"n\". Do not ever add extra information or stray from this structure.\n",
    "\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(META_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_PROMPT = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Choices: \n",
    "{choices}\n",
    "\"\"\"\n",
    "\n",
    "human_baseline_message_prompt = HumanMessagePromptTemplate.from_template(BASELINE_PROMPT)\n",
    "\n",
    "example_top_p_value = 0.7\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        \"top_p\": example_top_p_value,\n",
    "    },\n",
    "    max_tokens=32,\n",
    "    request_timeout=REQUEST_TIMEOUT,\n",
    ")\n",
    "\n",
    "forecaster_top_k_uncertainty = ForecasterTopKUncertainty(\n",
    "    chat=chat,\n",
    "    system_prompt=system_message_prompt,\n",
    "    human_prompt=human_baseline_message_prompt,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "When will Russia and Ukraine sign or announce an agreement to end the current conflict in Ukraine?\n",
      "\n",
      "Choices:\n",
      "A - Before 1 June 2022\n",
      "B - Between 1 June 2022 and 31 July 2022\n",
      "C - Between 1 August 2022 and 30 September 2022\n",
      "D - Between 1 October 2022 and 30 November 2022\n",
      "E - Not before 1 December 2022\n",
      "\n",
      "Crowd:\n",
      "Majority: 0.2\n",
      "Acceptable range: (0.1, 0.3)\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 25\n",
    "sample_question = test_df.iloc[sample_idx][\"question\"]\n",
    "sample_choices = test_df.iloc[sample_idx][\"choices\"]\n",
    "\n",
    "describe_question_row_by_idx(test_df, sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_prediction': ['B - Between 1 June 2022 and 31 July 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'B - Between 1 June 2022 and 31 July 2022',\n",
       "  'C - Between 1 August 2022 and 30 September 2022'],\n",
       " 'top_k_predicted_answers': ['B',\n",
       "  'B',\n",
       "  'C',\n",
       "  'C',\n",
       "  'C',\n",
       "  'B',\n",
       "  'C',\n",
       "  'C',\n",
       "  'B',\n",
       "  'C',\n",
       "  'B',\n",
       "  'C',\n",
       "  'C',\n",
       "  'B',\n",
       "  'B',\n",
       "  'C',\n",
       "  'C',\n",
       "  'B',\n",
       "  'B',\n",
       "  'C'],\n",
       " 'predicted_answer': 'C'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster_top_k_uncertainty.get_llm_response(\n",
    "    vars={\n",
    "        \"question\": sample_question,\n",
    "        \"choices\": forecaster_top_k_uncertainty.format_choices(sample_choices)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example question: Will Bashar al-Assad cease to be President of Syria before 1 March 2017?\n",
      "Example choices: ['yes', 'no']\n",
      "Example answer: no\n",
      "Formated example answer: B\n",
      "\n",
      "Example exchange:\n",
      "content='\\nQuestion: Will Bashar al-Assad cease to be President of Syria before 1 March 2017?\\n\\nChoices: \\nA - yes\\nB - no\\n' additional_kwargs={} example=False\n",
      "content='B' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "example_question = sample_df.iloc[example_idx][\"question\"]\n",
    "example_choices = sample_df.iloc[example_idx][\"choices\"]\n",
    "example_answer = sample_df.iloc[example_idx][\"answer\"]\n",
    "formatted_example_answer = forecaster_top_k_uncertainty.yes_no_to_letter(example_answer)\n",
    "\n",
    "print(f\"Example question: {example_question}\")\n",
    "print(f\"Example choices: {example_choices}\")\n",
    "print(f\"Example answer: {example_answer}\")\n",
    "print(f\"Formated example answer: {formatted_example_answer}\")\n",
    "print(\"\")\n",
    "\n",
    "example_exchange = forecaster_top_k_uncertainty.generate_example(\n",
    "    question=example_question,\n",
    "    choices=example_choices,\n",
    "    answer=example_answer\n",
    ")\n",
    "\n",
    "print(\"Example exchange:\")\n",
    "for message in example_exchange:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_prediction': [\"I'm sorry, but I cannot provide an accurate answer to this question as it involves predicting a future event that is subject to a wide range of complex political,\",\n",
       "  'A',\n",
       "  \"I'm sorry, but I don't have enough information to make an accurate prediction.\",\n",
       "  \"I'm sorry, but I cannot provide a reliable answer to this question.\",\n",
       "  \"I'm sorry, but I don't have enough information to provide an accurate answer.\",\n",
       "  '+++ \\nBackground Research:\\nThe conflict in Ukraine began in 2014 when Russia annexed Crimea from Ukraine and backed separatist rebels in eastern Ukraine. Several cease',\n",
       "  \"I'm sorry, but as an AI language model, I don't have access to up-to-date information or current events. I cannot accurately predict the future or\",\n",
       "  \"I'm sorry, but I don't have enough information to make an accurate prediction.\",\n",
       "  \"I'm sorry, but I don't have enough information to provide an accurate answer. Can you please provide me with some background research?\",\n",
       "  \"I'm sorry, but I don't have enough information to provide a confident answer.\",\n",
       "  '+++ \\nBackground Research:\\nThe conflict between Russia and Ukraine started in 2014 when Russia annexed Crimea. The conflict has since escalated into a low-intensity',\n",
       "  \"I'm sorry, but I don't have enough information to provide an accurate answer. Can you provide me with some background research or context to better understand the question\",\n",
       "  'A',\n",
       "  'A',\n",
       "  \"I'm sorry, but I don't have enough information to provide a confident answer.\",\n",
       "  \"I'm sorry, but I cannot provide an answer without background research. Could you please provide me with some useful information about the question?\",\n",
       "  'A',\n",
       "  \"I'm sorry, but I cannot provide an answer to this question as it is impossible to predict with certainty when Russia and Ukraine will sign or announce an agreement to\",\n",
       "  '+++ \\nBackground Research:\\nThe conflict between Russia and Ukraine started in 2014 after the Ukrainian Revolution, which led to the annexation of Crimea by Russia and',\n",
       "  'I am sorry, but I do not have enough information to provide an answer. Can you please provide me with some background research?'],\n",
       " 'top_k_predicted_answers': ['I',\n",
       "  'A',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  None,\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  None,\n",
       "  'I',\n",
       "  'A',\n",
       "  'A',\n",
       "  'I',\n",
       "  'I',\n",
       "  'A',\n",
       "  'I',\n",
       "  None,\n",
       "  'I'],\n",
       " 'predicted_answer': 'I'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster_top_k_uncertainty.get_llm_response_with_example(\n",
    "    vars={\n",
    "        \"question\": sample_question,\n",
    "        \"choices\": forecaster_top_k_uncertainty.format_choices(sample_choices)\n",
    "    },\n",
    "    examples=example_exchange\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_EXAMPLES_PER_QUESTION = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  1%|          | 1/98 [01:24<2:16:08, 84.21s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  2%|▏         | 2/98 [02:42<2:09:17, 80.81s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  3%|▎         | 3/98 [03:38<1:49:46, 69.33s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  4%|▍         | 4/98 [04:14<1:28:07, 56.25s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  5%|▌         | 5/98 [05:31<1:39:00, 63.88s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  6%|▌         | 6/98 [06:06<1:22:50, 54.03s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  7%|▋         | 7/98 [06:40<1:11:42, 47.28s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  8%|▊         | 8/98 [08:21<1:36:36, 64.40s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  9%|▉         | 9/98 [09:05<1:26:22, 58.23s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 11%|█         | 11/98 [10:21<1:06:05, 45.58s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 13%|█▎        | 13/98 [11:11<48:10, 34.00s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 14%|█▍        | 14/98 [12:39<1:10:29, 50.35s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 15%|█▌        | 15/98 [13:14<1:03:10, 45.67s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 16%|█▋        | 16/98 [13:50<58:37, 42.90s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 17%|█▋        | 17/98 [14:45<1:02:48, 46.52s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 19%|█▉        | 19/98 [16:02<53:07, 40.35s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 20%|██        | 20/98 [17:07<1:01:55, 47.63s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 22%|██▏       | 22/98 [19:05<1:02:26, 49.30s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 23%|██▎       | 23/98 [20:01<1:04:24, 51.53s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 24%|██▍       | 24/98 [21:24<1:14:54, 60.74s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 26%|██▌       | 25/98 [24:14<1:53:51, 93.58s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 27%|██▋       | 26/98 [25:33<1:47:04, 89.22s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 28%|██▊       | 27/98 [26:13<1:28:11, 74.53s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 29%|██▊       | 28/98 [27:17<1:23:09, 71.28s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 30%|██▉       | 29/98 [28:33<1:23:42, 72.79s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 32%|███▏      | 31/98 [30:40<1:12:30, 64.94s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 33%|███▎      | 32/98 [33:29<1:45:29, 95.89s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 34%|███▎      | 33/98 [34:06<1:24:46, 78.26s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 35%|███▍      | 34/98 [34:49<1:12:13, 67.71s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 37%|███▋      | 36/98 [35:54<50:31, 48.90s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 38%|███▊      | 37/98 [36:38<48:20, 47.55s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 39%|███▉      | 38/98 [38:43<1:10:33, 70.56s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 40%|███▉      | 39/98 [39:39<1:05:04, 66.18s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 41%|████      | 40/98 [40:35<1:00:58, 63.08s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 42%|████▏     | 41/98 [41:35<59:15, 62.38s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 43%|████▎     | 42/98 [42:31<56:29, 60.53s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 45%|████▍     | 44/98 [44:09<46:45, 51.96s/it]  Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 46%|████▌     | 45/98 [45:07<47:22, 53.63s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 48%|████▊     | 47/98 [46:18<36:07, 42.50s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 49%|████▉     | 48/98 [47:59<50:00, 60.01s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 50%|█████     | 49/98 [49:37<58:17, 71.37s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 51%|█████     | 50/98 [50:11<48:08, 60.19s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 52%|█████▏    | 51/98 [51:08<46:32, 59.42s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 54%|█████▍    | 53/98 [52:16<33:24, 44.53s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 55%|█████▌    | 54/98 [53:58<45:08, 61.55s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 56%|█████▌    | 55/98 [54:57<43:45, 61.05s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 57%|█████▋    | 56/98 [55:54<41:42, 59.58s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 58%|█████▊    | 57/98 [56:49<39:48, 58.26s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 59%|█████▉    | 58/98 [58:07<42:51, 64.29s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 60%|██████    | 59/98 [59:03<40:02, 61.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n",
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 63%|██████▎   | 62/98 [1:00:49<28:10, 46.97s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 64%|██████▍   | 63/98 [1:02:28<33:45, 57.88s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 65%|██████▌   | 64/98 [1:04:06<38:06, 67.25s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 66%|██████▋   | 65/98 [1:05:01<35:19, 64.23s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 67%|██████▋   | 66/98 [1:06:39<38:56, 73.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 69%|██████▉   | 68/98 [1:07:13<24:30, 49.01s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 70%|███████   | 69/98 [1:08:33<27:10, 56.24s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 71%|███████▏  | 70/98 [1:09:07<23:37, 50.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 73%|███████▎  | 72/98 [1:09:45<16:14, 37.49s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 74%|███████▍  | 73/98 [1:10:40<17:16, 41.47s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 76%|███████▌  | 74/98 [1:12:12<21:34, 53.94s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 77%|███████▋  | 75/98 [1:13:25<22:34, 58.91s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 80%|███████▉  | 78/98 [1:14:32<11:34, 34.70s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 81%|████████  | 79/98 [1:15:51<15:02, 47.51s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 82%|████████▏ | 80/98 [1:16:57<15:53, 52.95s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 83%|████████▎ | 81/98 [1:18:36<18:53, 66.67s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 84%|████████▎ | 82/98 [1:20:17<20:25, 76.59s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 85%|████████▍ | 83/98 [1:21:45<20:00, 80.06s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 86%|████████▌ | 84/98 [1:23:20<19:46, 84.72s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 87%|████████▋ | 85/98 [1:24:52<18:47, 86.69s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 88%|████████▊ | 86/98 [1:27:34<21:52, 109.40s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 90%|████████▉ | 88/98 [1:28:28<11:07, 66.73s/it] Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 91%|█████████ | 89/98 [1:29:02<08:32, 56.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 93%|█████████▎| 91/98 [1:29:59<05:06, 43.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 1\n",
      "Cannot take a larger sample than population when 'replace=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 95%|█████████▍| 93/98 [1:30:55<03:06, 37.39s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 96%|█████████▌| 94/98 [1:32:41<03:30, 52.70s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 97%|█████████▋| 95/98 [1:34:22<03:12, 64.17s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 98%|█████████▊| 96/98 [1:36:02<02:26, 73.46s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "100%|██████████| 98/98 [1:37:03<00:00, 59.43s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    id = row[\"id\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    qtype = row[\"qtype\"]\n",
    "    tags = row[\"tags\"]\n",
    "    \n",
    "    error_count = 0\n",
    "    try:\n",
    "        example_uncertainty_results = forecaster_top_k_uncertainty.generate_n_examples_with_uncertainty(\n",
    "            question=question,\n",
    "            choices=choices,\n",
    "            example_df=sample_df,\n",
    "            qtype=qtype,\n",
    "            tags=tags,\n",
    "            n=NUMBER_EXAMPLES_PER_QUESTION,\n",
    "            random=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"Error count: {error_count}\")\n",
    "        print(e)\n",
    "        example_uncertainty_results = {}\n",
    "    \n",
    "    example_data.append({\n",
    "        \"id\": id,\n",
    "        \"question\": question,\n",
    "        \"examples\": example_uncertainty_results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G2124</td>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>[{'example_question': 'Between 1 January 2018 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G2125</td>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>[{'example_question': 'Will the EU amend its O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G2141</td>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>[{'example_question': 'Between 18 October and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0  G2124  Will there be a complex coordinated terrorist ...   \n",
       "1  G2125  Before 1 September 2022, will Egypt, Ethiopia,...   \n",
       "2  G2141  Before 1 January 2023, will the Taipei Economi...   \n",
       "\n",
       "                                            examples  \n",
       "0  [{'example_question': 'Between 1 January 2018 ...  \n",
       "1  [{'example_question': 'Will the EU amend its O...  \n",
       "2  [{'example_question': 'Between 18 October and ...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df = pd.DataFrame(example_data)\n",
    "\n",
    "example_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example_data_name(df, model_name: str = MODEL_NAME,  description: str = None):\n",
    "    \"\"\"\n",
    "    Builds a name for the example data file.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A dataframe with columns \"question\" and \"examples\".\n",
    "        model_name (str): The name of the model.\n",
    "        description (str): A description of the example data.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    df_size = df.shape[0]\n",
    "    \n",
    "    save_name = f\"{model_name}_{timestamp}_examples-{df_size}\"\n",
    "    \n",
    "    if description is not None:\n",
    "        save_name += f\"_{description}\"\n",
    "\n",
    "    extension = \"json\"\n",
    "    save_name += f\".{extension}\"\n",
    "    \n",
    "    return save_name\n",
    "\n",
    "def save_example_data_json(df, model_name=MODEL_NAME, data_dir=\"../data\", save_path=None):\n",
    "    \"\"\"\n",
    "    Saves a example dataframe to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A dataframe with columns \"question\" and \"examples\".\n",
    "        model_name (str): The name of the model.\n",
    "        data_dir (str): The directory to save the JSON file to.\n",
    "        save_path (str): The path to save the JSON file to.\n",
    "    \"\"\"\n",
    "    if save_path is None:\n",
    "        save_name = build_example_data_name(df, model_name=model_name)\n",
    "        save_path = os.path.join(data_dir, save_name)\n",
    "        \n",
    "    df.to_json(save_path, orient=\"records\", indent=4)\n",
    "    print(f\"Saved {df.shape[0]} examples to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/gpt-35-turbo_2023-08-06-09-43-35_examples-98_main_example_set.json'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_SAVE_NAME = build_example_data_name(example_df, MODEL_NAME, \"main_example_set\")\n",
    "EXAMPLE_SAVE_PATH = os.path.join(DATA_DIR, EXAMPLE_SAVE_NAME)\n",
    "\n",
    "EXAMPLE_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 98 examples to: ../data/gpt-35-turbo_2023-08-06-09-43-35_examples-98_main_example_set.json\n"
     ]
    }
   ],
   "source": [
    "save_example_data_json(example_df, save_path=EXAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_example(example_list: list) -> dict:\n",
    "    \"\"\"\n",
    "    Chooses the best example from a list of examples, where quality is \n",
    "    determined by low uncertainty. If there are multiple examples with the\n",
    "    lowest uncertainty, then randomly returns one of them.\n",
    "    \"\"\"\n",
    "    min_uncertainty = min([example[\"uncertainty\"] for example in example_list])\n",
    "    best_examples = [example for example in example_list if example[\"uncertainty\"] == min_uncertainty]\n",
    "    best_example = np.random.choice(best_examples)\n",
    "    return best_example\n",
    "\n",
    "def fill_examples(all_examples, filled_examples):\n",
    "    \"\"\"\n",
    "    Fills in examples for rows that have no examples.\n",
    "    \"\"\"\n",
    "    new_examples = []\n",
    "    for examples in all_examples:\n",
    "        if examples == [] or examples == {}:\n",
    "            new_examples.append(random.choice(filled_examples))\n",
    "        else:\n",
    "            new_examples.append(examples)\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill any empty examples\n",
    "empty_examples = [ex for ex in example_df[\"examples\"] if len(ex) == 0]\n",
    "filled_examples = [ex for ex in example_df[\"examples\"] if len(ex) > 0]\n",
    "all_examples = fill_examples(example_df[\"examples\"], filled_examples)\n",
    "\n",
    "example_df[\"examples\"] = all_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to example_df with the best example\n",
    "example_df[\"best_example\"] = example_df[\"examples\"].apply(choose_best_example)\n",
    "\n",
    "# add a column with the lowest uncertainty\n",
    "example_df[\"lowest_uncertainty\"] = example_df[\"examples\"].apply(\n",
    "    lambda x: min([example[\"uncertainty\"] for example in x])\n",
    ")\n",
    "\n",
    "# add a column with the highest uncertainty\n",
    "example_df[\"highest_uncertainty\"] = example_df[\"examples\"].apply(\n",
    "    lambda x: max([example[\"uncertainty\"] for example in x])\n",
    ")\n",
    "\n",
    "# add a column with the average uncertainty\n",
    "example_df[\"average_uncertainty\"] = example_df[\"examples\"].apply(\n",
    "    lambda x: np.mean([example[\"uncertainty\"] for example in x])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>examples</th>\n",
       "      <th>best_example</th>\n",
       "      <th>lowest_uncertainty</th>\n",
       "      <th>highest_uncertainty</th>\n",
       "      <th>average_uncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G2124</td>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>[{'example_question': 'Between 1 January 2018 ...</td>\n",
       "      <td>{'example_question': 'Will the government of C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G2125</td>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>[{'example_question': 'Will the EU amend its O...</td>\n",
       "      <td>{'example_question': 'Will either Turkey or Ru...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G2141</td>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>[{'example_question': 'Between 18 October and ...</td>\n",
       "      <td>{'example_question': 'Will the New START treat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0  G2124  Will there be a complex coordinated terrorist ...   \n",
       "1  G2125  Before 1 September 2022, will Egypt, Ethiopia,...   \n",
       "2  G2141  Before 1 January 2023, will the Taipei Economi...   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [{'example_question': 'Between 1 January 2018 ...   \n",
       "1  [{'example_question': 'Will the EU amend its O...   \n",
       "2  [{'example_question': 'Between 18 October and ...   \n",
       "\n",
       "                                        best_example  lowest_uncertainty  \\\n",
       "0  {'example_question': 'Will the government of C...                 0.0   \n",
       "1  {'example_question': 'Will either Turkey or Ru...                 0.0   \n",
       "2  {'example_question': 'Will the New START treat...                 0.0   \n",
       "\n",
       "   highest_uncertainty  average_uncertainty  \n",
       "0                 0.45                0.082  \n",
       "1                 0.50                0.104  \n",
       "2                 0.35                0.040  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average, lowest, and highest uncertainty\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle(\"Uncertainty of examples per question\")\n",
    "\n",
    "sns.histplot(\n",
    "    data=example_df,\n",
    "    x=\"average_uncertainty\",\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Average\")\n",
    "axes[0].set_xlabel(\"Uncertainty\")\n",
    "\n",
    "sns.histplot(\n",
    "    data=example_df,\n",
    "    x=\"lowest_uncertainty\",\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Lowest\")\n",
    "axes[1].set_xlabel(\"Uncertainty\")\n",
    "\n",
    "sns.histplot(\n",
    "    data=example_df,\n",
    "    x=\"highest_uncertainty\",\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Highest\")\n",
    "axes[2].set_xlabel(\"Uncertainty\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 98 examples to: ../data/gpt-35-turbo_2023-08-06-09-43-35_examples-98_main_example_set.json\n"
     ]
    }
   ],
   "source": [
    "save_example_data_json(example_df, save_path=EXAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Context to Best Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will the government of Colombia and the FARC renegotiate a peace agreement before 1 March 2017?\n"
     ]
    }
   ],
   "source": [
    "sample_question = example_df.iloc[0][\"best_example\"][\"example_question\"]\n",
    "print(sample_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/98 [00:46<18:05, 11.55s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "  8%|▊         | 8/98 [01:56<22:05, 14.73s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 12%|█▏        | 12/98 [03:10<23:08, 16.14s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 20%|██        | 20/98 [05:23<18:02, 13.88s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 38%|███▊      | 37/98 [09:20<12:39, 12.45s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 45%|████▍     | 44/98 [10:56<10:15, 11.39s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 74%|███████▍  | 73/98 [17:15<04:58, 11.93s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 82%|████████▏ | 80/98 [18:52<03:44, 12.45s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      " 99%|█████████▉| 97/98 [22:37<00:11, 11.35s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='capstone-openai.openai.azure.com', port=443): Read timed out. (read timeout=20.0).\n",
      "100%|██████████| 98/98 [23:10<00:00, 14.19s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(example_df.iterrows(), total=example_df.shape[0]):\n",
    "    best_example = row[\"best_example\"]\n",
    "    best_example_question = best_example[\"example_question\"]\n",
    "\n",
    "    error_count = 0\n",
    "    try:\n",
    "        context_result = get_background_research(\n",
    "            best_example_question,\n",
    "            chat=context_chat,\n",
    "            prompt=context_chat_prompt,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        print(f\"Error count: {error_count}\")\n",
    "        print(e)\n",
    "        context_result = \"\"\n",
    "    \n",
    "    best_example[\"example_context\"] = context_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>examples</th>\n",
       "      <th>best_example</th>\n",
       "      <th>lowest_uncertainty</th>\n",
       "      <th>highest_uncertainty</th>\n",
       "      <th>average_uncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G2124</td>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>[{'example_question': 'Between 1 January 2018 ...</td>\n",
       "      <td>{'example_question': 'Will the government of C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G2125</td>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>[{'example_question': 'Will the EU amend its O...</td>\n",
       "      <td>{'example_question': 'Will either Turkey or Ru...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G2141</td>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>[{'example_question': 'Between 18 October and ...</td>\n",
       "      <td>{'example_question': 'Will the New START treat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           question  \\\n",
       "0  G2124  Will there be a complex coordinated terrorist ...   \n",
       "1  G2125  Before 1 September 2022, will Egypt, Ethiopia,...   \n",
       "2  G2141  Before 1 January 2023, will the Taipei Economi...   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [{'example_question': 'Between 1 January 2018 ...   \n",
       "1  [{'example_question': 'Will the EU amend its O...   \n",
       "2  [{'example_question': 'Between 18 October and ...   \n",
       "\n",
       "                                        best_example  lowest_uncertainty  \\\n",
       "0  {'example_question': 'Will the government of C...                 0.0   \n",
       "1  {'example_question': 'Will either Turkey or Ru...                 0.0   \n",
       "2  {'example_question': 'Will the New START treat...                 0.0   \n",
       "\n",
       "   highest_uncertainty  average_uncertainty  \n",
       "0                 0.45                0.082  \n",
       "1                 0.50                0.104  \n",
       "2                 0.35                0.040  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 98 examples to: ../data/gpt-35-turbo_2023-08-06-09-43-35_examples-98_main_example_set.json\n"
     ]
    }
   ],
   "source": [
    "save_example_data_json(example_df, save_path=EXAMPLE_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Test Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>background</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>close_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_links</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>forecaster_count</th>\n",
       "      <th>answer</th>\n",
       "      <th>...</th>\n",
       "      <th>majority_pred</th>\n",
       "      <th>pred_taken</th>\n",
       "      <th>acceptable_pred_lower_boundary</th>\n",
       "      <th>acceptable_pred_upper_boundary</th>\n",
       "      <th>context</th>\n",
       "      <th>examples</th>\n",
       "      <th>best_example</th>\n",
       "      <th>lowest_uncertainty</th>\n",
       "      <th>highest_uncertainty</th>\n",
       "      <th>average_uncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>G2124</td>\n",
       "      <td>CCTAs are a major concern for law enforcement ...</td>\n",
       "      <td>2021-09-17 15:49:41.402000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Society, Security and Conflict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>340.0</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Complex coordinated terrorist att...</td>\n",
       "      <td>[{'example_question': 'Between 1 January 2018 ...</td>\n",
       "      <td>{'example_question': 'Will the government of C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>G2125</td>\n",
       "      <td>Tensions between Egypt, Ethiopia, and Sudan co...</td>\n",
       "      <td>2021-09-17 15:49:44.237000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, Security and Conflict, Non-US...</td>\n",
       "      <td>[https://www.securitycouncilreport.org/whatsin...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>59</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Egypt: a country in North Africa,...</td>\n",
       "      <td>[{'example_question': 'Will the EU amend its O...</td>\n",
       "      <td>{'example_question': 'Will either Turkey or Ru...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>G2141</td>\n",
       "      <td>Recent media reporting indicates discussions r...</td>\n",
       "      <td>2021-09-30 14:38:57.362000+00:00</td>\n",
       "      <td>2023-01-01 08:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, US Politics, Security and Con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>188.0</td>\n",
       "      <td>51</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.36</td>\n",
       "      <td>Entities:\\n- Taipei Economic and Cultural Repr...</td>\n",
       "      <td>[{'example_question': 'Between 18 October and ...</td>\n",
       "      <td>{'example_question': 'Will the New START treat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     id  \\\n",
       "0  Will there be a complex coordinated terrorist ...  G2124   \n",
       "1  Before 1 September 2022, will Egypt, Ethiopia,...  G2125   \n",
       "2  Before 1 January 2023, will the Taipei Economi...  G2141   \n",
       "\n",
       "                                          background  \\\n",
       "0  CCTAs are a major concern for law enforcement ...   \n",
       "1  Tensions between Egypt, Ethiopia, and Sudan co...   \n",
       "2  Recent media reporting indicates discussions r...   \n",
       "\n",
       "                      publish_time                 close_time  \\\n",
       "0 2021-09-17 15:49:41.402000+00:00  2022-09-01 07:01:00+00:00   \n",
       "1 2021-09-17 15:49:44.237000+00:00  2022-09-01 07:01:00+00:00   \n",
       "2 2021-09-30 14:38:57.362000+00:00  2023-01-01 08:01:00+00:00   \n",
       "\n",
       "                                                tags  \\\n",
       "0                   [Society, Security and Conflict]   \n",
       "1  [Foreign Policy, Security and Conflict, Non-US...   \n",
       "2  [Foreign Policy, US Politics, Security and Con...   \n",
       "\n",
       "                                        source_links  prediction_count  \\\n",
       "0                                                 []             340.0   \n",
       "1  [https://www.securitycouncilreport.org/whatsin...             166.0   \n",
       "2                                                 []             188.0   \n",
       "\n",
       "   forecaster_count answer  ... majority_pred pred_taken  \\\n",
       "0               111   None  ...          0.05       0.05   \n",
       "1                59   None  ...          0.05       0.05   \n",
       "2                51   None  ...          0.16       0.16   \n",
       "\n",
       "  acceptable_pred_lower_boundary acceptable_pred_upper_boundary  \\\n",
       "0                          -0.15                           0.25   \n",
       "1                          -0.15                           0.25   \n",
       "2                          -0.04                           0.36   \n",
       "\n",
       "                                             context  \\\n",
       "0  Entities:\\n- Complex coordinated terrorist att...   \n",
       "1  Entities:\\n- Egypt: a country in North Africa,...   \n",
       "2  Entities:\\n- Taipei Economic and Cultural Repr...   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [{'example_question': 'Between 1 January 2018 ...   \n",
       "1  [{'example_question': 'Will the EU amend its O...   \n",
       "2  [{'example_question': 'Between 18 October and ...   \n",
       "\n",
       "                                        best_example  lowest_uncertainty  \\\n",
       "0  {'example_question': 'Will the government of C...                 0.0   \n",
       "1  {'example_question': 'Will either Turkey or Ru...                 0.0   \n",
       "2  {'example_question': 'Will the New START treat...                 0.0   \n",
       "\n",
       "   highest_uncertainty  average_uncertainty  \n",
       "0                 0.45                0.082  \n",
       "1                 0.50                0.104  \n",
       "2                 0.35                0.040  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.merge(\n",
    "    example_df,\n",
    "    on=[\"id\", \"question\"]\n",
    ")\n",
    "\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (98, 26)\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.drop_duplicates(subset=[\"id\", \"question\"])\n",
    "\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test dataframe to: ../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json\n"
     ]
    }
   ],
   "source": [
    "save_test_df(test_df, TEST_DATA_FILE_NAME, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add External Context\n",
    "\n",
    "Add context from pre-made external dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_context_path = \"../results/GR_0.3_all_forecasts_baseline_df_20230727.csv\"\n",
    "external_context_df = pd.read_csv(external_context_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of external context data: (98, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of external context data: {external_context_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>qtype</th>\n",
       "      <th>choices_clean</th>\n",
       "      <th>answer</th>\n",
       "      <th>avg_forecast</th>\n",
       "      <th>crowd</th>\n",
       "      <th>question_context</th>\n",
       "      <th>LLM_answers</th>\n",
       "      <th>LLM_answers_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>G2124</th>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>t/f</td>\n",
       "      <td>{'A': 'yes', 'B': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>[{'forecast': 0.05}]</td>\n",
       "      <td>The United States remains in a heightened thre...</td>\n",
       "      <td>['B - no.', 'B - no.', 'B - no.', 'B - no.', '...</td>\n",
       "      <td>['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2125</th>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>t/f</td>\n",
       "      <td>{'A': 'yes', 'B': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>[{'forecast': 0.05}]</td>\n",
       "      <td>As of August 2021, the foreign ministers of Eg...</td>\n",
       "      <td>['A - yes.', 'A - yes.', 'A - yes.', 'A - yes....</td>\n",
       "      <td>['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2141</th>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>t/f</td>\n",
       "      <td>{'A': 'yes', 'B': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>[{'forecast': 0.16}]</td>\n",
       "      <td>The Taipei Economic and Cultural Representativ...</td>\n",
       "      <td>['A - Yes.', 'A - yes.', 'A - yes', 'B - no.',...</td>\n",
       "      <td>['A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2164</th>\n",
       "      <td>Will there be a lethal confrontation between t...</td>\n",
       "      <td>t/f</td>\n",
       "      <td>{'A': 'yes', 'B': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>[{'forecast': 0.1}]</td>\n",
       "      <td>The border dispute between India and China has...</td>\n",
       "      <td>['B - no.', 'B - no.', 'B - no', 'B - no.', 'B...</td>\n",
       "      <td>['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G2169</th>\n",
       "      <td>Will Taiwan publicly accuse the People's Repub...</td>\n",
       "      <td>t/f</td>\n",
       "      <td>{'A': 'yes', 'B': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>[{'forecast': 0.3}]</td>\n",
       "      <td>Taiwan and China have a long-standing conflict...</td>\n",
       "      <td>['B - no.', 'B - no.', 'B - no', 'B - no.', 'B...</td>\n",
       "      <td>['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question qtype  \\\n",
       "id                                                               \n",
       "G2124  Will there be a complex coordinated terrorist ...   t/f   \n",
       "G2125  Before 1 September 2022, will Egypt, Ethiopia,...   t/f   \n",
       "G2141  Before 1 January 2023, will the Taipei Economi...   t/f   \n",
       "G2164  Will there be a lethal confrontation between t...   t/f   \n",
       "G2169  Will Taiwan publicly accuse the People's Repub...   t/f   \n",
       "\n",
       "                 choices_clean  answer avg_forecast                 crowd  \\\n",
       "id                                                                          \n",
       "G2124  {'A': 'yes', 'B': 'no'}     NaN            B  [{'forecast': 0.05}]   \n",
       "G2125  {'A': 'yes', 'B': 'no'}     NaN            B  [{'forecast': 0.05}]   \n",
       "G2141  {'A': 'yes', 'B': 'no'}     NaN            B  [{'forecast': 0.16}]   \n",
       "G2164  {'A': 'yes', 'B': 'no'}     NaN            B   [{'forecast': 0.1}]   \n",
       "G2169  {'A': 'yes', 'B': 'no'}     NaN            B   [{'forecast': 0.3}]   \n",
       "\n",
       "                                        question_context  \\\n",
       "id                                                         \n",
       "G2124  The United States remains in a heightened thre...   \n",
       "G2125  As of August 2021, the foreign ministers of Eg...   \n",
       "G2141  The Taipei Economic and Cultural Representativ...   \n",
       "G2164  The border dispute between India and China has...   \n",
       "G2169  Taiwan and China have a long-standing conflict...   \n",
       "\n",
       "                                             LLM_answers  \\\n",
       "id                                                         \n",
       "G2124  ['B - no.', 'B - no.', 'B - no.', 'B - no.', '...   \n",
       "G2125  ['A - yes.', 'A - yes.', 'A - yes.', 'A - yes....   \n",
       "G2141  ['A - Yes.', 'A - yes.', 'A - yes', 'B - no.',...   \n",
       "G2164  ['B - no.', 'B - no.', 'B - no', 'B - no.', 'B...   \n",
       "G2169  ['B - no.', 'B - no.', 'B - no', 'B - no.', 'B...   \n",
       "\n",
       "                                       LLM_answers_clean  \n",
       "id                                                        \n",
       "G2124  ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...  \n",
       "G2125  ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', ...  \n",
       "G2141  ['A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', ...  \n",
       "G2164  ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...  \n",
       "G2169  ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_context_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Will Kassym-Jomart Tokayev either flee Kazakhstan or cease to be its president before 7 July 2022?\n",
      "\n",
      "External Context:  Kassym-Jomart Tokayev is the current president of Kazakhstan, having taken office in 2019 after the resignation of Nursultan Nazarbayev. There have been recent riots in Kazakhstan due to the government's removal of fuel subsidies, but it is unclear whether or not Tokayev will flee or cease to be president before July 7, 2022.\n"
     ]
    }
   ],
   "source": [
    "sample_row_idx = 10\n",
    "sample_question = external_context_df.iloc[sample_row_idx][\"question\"]\n",
    "sample_external_context = external_context_df.iloc[sample_row_idx][\"question_context\"]\n",
    "\n",
    "print(\"Question: \", sample_question)\n",
    "print(\"\")\n",
    "print(\"External Context: \", sample_external_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_context</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The United States remains in a heightened thre...</td>\n",
       "      <td>G2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As of August 2021, the foreign ministers of Eg...</td>\n",
       "      <td>G2125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Taipei Economic and Cultural Representativ...</td>\n",
       "      <td>G2141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The border dispute between India and China has...</td>\n",
       "      <td>G2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taiwan and China have a long-standing conflict...</td>\n",
       "      <td>G2169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    external_context     id\n",
       "0  The United States remains in a heightened thre...  G2124\n",
       "1  As of August 2021, the foreign ministers of Eg...  G2125\n",
       "2  The Taipei Economic and Cultural Representativ...  G2141\n",
       "3  The border dispute between India and China has...  G2164\n",
       "4  Taiwan and China have a long-standing conflict...  G2169"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_context_df = external_context_df.reset_index()\n",
    "external_context_df = external_context_df[['question_context', 'id']]\n",
    "external_context_df.columns = ['external_context', 'id']\n",
    "external_context_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>background</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>close_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>source_links</th>\n",
       "      <th>prediction_count</th>\n",
       "      <th>forecaster_count</th>\n",
       "      <th>answer</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_taken</th>\n",
       "      <th>acceptable_pred_lower_boundary</th>\n",
       "      <th>acceptable_pred_upper_boundary</th>\n",
       "      <th>context</th>\n",
       "      <th>examples</th>\n",
       "      <th>best_example</th>\n",
       "      <th>lowest_uncertainty</th>\n",
       "      <th>highest_uncertainty</th>\n",
       "      <th>average_uncertainty</th>\n",
       "      <th>external_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Will there be a complex coordinated terrorist ...</td>\n",
       "      <td>G2124</td>\n",
       "      <td>CCTAs are a major concern for law enforcement ...</td>\n",
       "      <td>2021-09-17 15:49:41.402000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Society, Security and Conflict]</td>\n",
       "      <td>[]</td>\n",
       "      <td>340.0</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Complex coordinated terrorist att...</td>\n",
       "      <td>[{'example_question': 'Between 1 January 2018 ...</td>\n",
       "      <td>{'example_question': 'Will the government of C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.082</td>\n",
       "      <td>The United States remains in a heightened thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before 1 September 2022, will Egypt, Ethiopia,...</td>\n",
       "      <td>G2125</td>\n",
       "      <td>Tensions between Egypt, Ethiopia, and Sudan co...</td>\n",
       "      <td>2021-09-17 15:49:44.237000+00:00</td>\n",
       "      <td>2022-09-01 07:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, Security and Conflict, Non-US...</td>\n",
       "      <td>[https://www.securitycouncilreport.org/whatsin...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>59</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>Entities:\\n- Egypt: a country in North Africa,...</td>\n",
       "      <td>[{'example_question': 'Will the EU amend its O...</td>\n",
       "      <td>{'example_question': 'Will either Turkey or Ru...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.104</td>\n",
       "      <td>As of August 2021, the foreign ministers of Eg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before 1 January 2023, will the Taipei Economi...</td>\n",
       "      <td>G2141</td>\n",
       "      <td>Recent media reporting indicates discussions r...</td>\n",
       "      <td>2021-09-30 14:38:57.362000+00:00</td>\n",
       "      <td>2023-01-01 08:01:00+00:00</td>\n",
       "      <td>[Foreign Policy, US Politics, Security and Con...</td>\n",
       "      <td>[]</td>\n",
       "      <td>188.0</td>\n",
       "      <td>51</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.36</td>\n",
       "      <td>Entities:\\n- Taipei Economic and Cultural Repr...</td>\n",
       "      <td>[{'example_question': 'Between 18 October and ...</td>\n",
       "      <td>{'example_question': 'Will the New START treat...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.040</td>\n",
       "      <td>The Taipei Economic and Cultural Representativ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     id  \\\n",
       "0  Will there be a complex coordinated terrorist ...  G2124   \n",
       "1  Before 1 September 2022, will Egypt, Ethiopia,...  G2125   \n",
       "2  Before 1 January 2023, will the Taipei Economi...  G2141   \n",
       "\n",
       "                                          background  \\\n",
       "0  CCTAs are a major concern for law enforcement ...   \n",
       "1  Tensions between Egypt, Ethiopia, and Sudan co...   \n",
       "2  Recent media reporting indicates discussions r...   \n",
       "\n",
       "                      publish_time                 close_time  \\\n",
       "0 2021-09-17 15:49:41.402000+00:00  2022-09-01 07:01:00+00:00   \n",
       "1 2021-09-17 15:49:44.237000+00:00  2022-09-01 07:01:00+00:00   \n",
       "2 2021-09-30 14:38:57.362000+00:00  2023-01-01 08:01:00+00:00   \n",
       "\n",
       "                                                tags  \\\n",
       "0                   [Society, Security and Conflict]   \n",
       "1  [Foreign Policy, Security and Conflict, Non-US...   \n",
       "2  [Foreign Policy, US Politics, Security and Con...   \n",
       "\n",
       "                                        source_links  prediction_count  \\\n",
       "0                                                 []             340.0   \n",
       "1  [https://www.securitycouncilreport.org/whatsin...             166.0   \n",
       "2                                                 []             188.0   \n",
       "\n",
       "   forecaster_count answer  ... pred_taken acceptable_pred_lower_boundary  \\\n",
       "0               111   None  ...       0.05                          -0.15   \n",
       "1                59   None  ...       0.05                          -0.15   \n",
       "2                51   None  ...       0.16                          -0.04   \n",
       "\n",
       "  acceptable_pred_upper_boundary  \\\n",
       "0                           0.25   \n",
       "1                           0.25   \n",
       "2                           0.36   \n",
       "\n",
       "                                             context  \\\n",
       "0  Entities:\\n- Complex coordinated terrorist att...   \n",
       "1  Entities:\\n- Egypt: a country in North Africa,...   \n",
       "2  Entities:\\n- Taipei Economic and Cultural Repr...   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [{'example_question': 'Between 1 January 2018 ...   \n",
       "1  [{'example_question': 'Will the EU amend its O...   \n",
       "2  [{'example_question': 'Between 18 October and ...   \n",
       "\n",
       "                                        best_example  lowest_uncertainty  \\\n",
       "0  {'example_question': 'Will the government of C...                 0.0   \n",
       "1  {'example_question': 'Will either Turkey or Ru...                 0.0   \n",
       "2  {'example_question': 'Will the New START treat...                 0.0   \n",
       "\n",
       "   highest_uncertainty  average_uncertainty  \\\n",
       "0                 0.45                0.082   \n",
       "1                 0.50                0.104   \n",
       "2                 0.35                0.040   \n",
       "\n",
       "                                    external_context  \n",
       "0  The United States remains in a heightened thre...  \n",
       "1  As of August 2021, the foreign ministers of Eg...  \n",
       "2  The Taipei Economic and Cultural Representativ...  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.merge(\n",
    "    external_context_df,\n",
    "    on=[\"id\"]\n",
    ")\n",
    "\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved test dataframe to: ../data/test_df-2023-08-06-07-42-24-size_98-filtered_test_data.json\n"
     ]
    }
   ],
   "source": [
    "save_test_df(test_df, TEST_DATA_FILE_NAME, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
